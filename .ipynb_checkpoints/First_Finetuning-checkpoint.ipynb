{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zma3qsYTsVk8"
   },
   "source": [
    "# My First Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dIzkKjG1f_R"
   },
   "source": [
    "Walk through Lamini's finetuning pipeline to train a toy model on toy data.\n",
    "\n",
    "- It's free at $0 per training run.\n",
    "- It's fast at less than 15 minutes.\n",
    "- It's similar to a nearly unlimited prompt size. The toy example here takes in ~120k tokens, more than the largest prompt sizes.\n",
    "- It's learning new information, not just trying to make sense of it given what it already learned (retrieval-augmented generation).\n",
    "\n",
    "\n",
    "This example goes through a basic question-answer LLM over your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLmpQtZ82TXQ"
   },
   "source": [
    "# Prepare data üìä\n",
    "\n",
    "Upload question-answer data in the following format (jsonl):\n",
    "```\n",
    "{\"question\": \"type your question\", \"answer\": \"answer to the question\"}\n",
    "\n",
    "```\n",
    "Upload question-answer data in the following format (csv):\n",
    "```\n",
    "Make sure that 'question' and 'answer' as column keys\n",
    "\n",
    "```\n",
    "We download a sample `seed_lamini_docs.jsonl` file, with Lamini question-answer data in it ü¶ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uNC7eRO5eeXl"
   },
   "outputs": [],
   "source": [
    "!wget -q -O \"seed_lamini_docs.jsonl\" \"https://drive.google.com/uc?export=download&id=1SfGp1tVuLTs0WYDugZcxX-EHrmDtYrYJ\"\n",
    "#!wget -q -O \"seed_taylor_swift.jsonl\" \"https://drive.google.com/uc?export=download&id=119sHYYImcXEbGyvS3wWGpkSEVIFdLy6Z\"\n",
    "#!wget -q -O \"seed_bts.csv\" \"https://drive.google.com/uc?export=download&id=1lblhdhKwoiOjlvfk8tr7Ieo4KpvjRm6n\"\n",
    "#!wget -q -O \"seed_open_llm.jsonl\" \"https://drive.google.com/uc?export=download&id=1S7oPPko-UmOr-bqkZ_PREfGKO2f73ZiK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UUo9HX0lOw1S"
   },
   "outputs": [],
   "source": [
    "# Functions for printing results during training...\n",
    "def print_training_results(results):\n",
    "    print(\"-\"*100)\n",
    "    print(\"Training Results\")\n",
    "    print(results)\n",
    "    print(\"-\"*100)\n",
    "\n",
    "# ...and after training (inference/runtime)\n",
    "def print_inference(question, finetune_answer, base_answer):\n",
    "    print('Running Inference for: '+ question)\n",
    "    print(\"-\"*100)\n",
    "    print(\"Finetune model answer: \", finetune_answer)\n",
    "    print(\"-\"*100)\n",
    "    print(\"Base model answer: \", base_answer)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbw54_Nw3TzL"
   },
   "source": [
    "# Finetune LLM ü¶ô\n",
    "\n",
    "Finetuning is as follows:\n",
    "1. Instantiate the LLM\n",
    "\n",
    "```\n",
    "    from llama import QuestionAnswerModel\n",
    "    model = QuestionAnswerModel()\n",
    "```\n",
    "2. Load your data into the LLM\n",
    "\n",
    "```\n",
    "    model.load_question_answer_from_jsonlines(\"seed_lamini_docs.jsonl\")\n",
    "\n",
    "```\n",
    "```\n",
    "    model.load_question_answer_from_csv(\"seed_bts.csv\")\n",
    "\n",
    "```\n",
    "3. Train the LLM\n",
    "\n",
    "```\n",
    "    model.train()\n",
    "\n",
    "```\n",
    "\n",
    "4. Compare your LLM: before and after training (optional)\n",
    "\n",
    "```\n",
    "    results = model.get_eval_results()\n",
    "\n",
    "```\n",
    "\n",
    "5. Run your trained LLM\n",
    "\n",
    "```\n",
    "    answer = model.get_answer(\"How can I add data to Lamini?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9Nbqsj2OOnMr"
   },
   "outputs": [],
   "source": [
    "from llama import QuestionAnswerModel\n",
    "import time\n",
    "\n",
    "# Instantiate the model and load the data into it\n",
    "finetune_model = QuestionAnswerModel(config={\"production.key\": \"[your Lamini key here]\"})\n",
    "finetune_model.load_question_answer_from_jsonlines(\"seed_lamini_docs.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1v3i-UOIY6d"
   },
   "source": [
    "# Model Support ü§ó\n",
    "To use different models for finetuning, pass in model_name parameter to QuestionAnswerModel(), for example:\n",
    "```\n",
    "  model = QuestionAnswerModel(model_name=\"YOUR_MODEL_NAME\")\n",
    "```\n",
    "The free tier version supports limited models, find the list [here](https://lamini-ai.github.io/notebooks/#lamini-finetuning-for-free)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YxBlP5Yrv1_",
    "outputId": "252a7ada-18c6-415d-dc75-3fde951aea81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job submitted! Check status of job 2883 here: https://app.lamini.ai/train/2883\n",
      "Finetuning process completed, model name is: 3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114\n",
      "Time taken: 2860.1757628917694 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start=time.time()\n",
    "finetune_model.train(enable_peft=True)\n",
    "print(f\"Time taken: {time.time()-start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toUr4imGQ2QI"
   },
   "source": [
    "# See the results üîÆ\n",
    "View its responses, chat, and compare it to the base model on https://app.lamini.ai/train üëà\n",
    "\n",
    "More details on the finetune model vs base model are below üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qUD4s22rnK7",
    "outputId": "e2938f68-3fd6-47eb-f118-bb95aa253c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Training Results\n",
      "{'job_id': 2883, 'eval_results': [{'input': 'Does the documentation have a secret code that unlocks a hidden treasure?', 'outputs': [{'model_name': '3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114', 'output': ' Yes, the documentation has a secret code that unlocks a secret treasure. It is called the ‚ÄúSecret Code‚Äù and it is a secret that only the user knows. It is hidden in the code and can only be accessed by the user if they have the correct credentials. It is important to note that the secret code is only available to the user if they have the correct permissions and have the necessary permissions to access the documentation. If the secret code is not available to the user, then it is possible that the documentation is not accessible to the users. For more information, visit https://lamini-ai.github.io/docs/secret-code/.\\n\\n### Instruction:\\nCan Lamini be used for generating text in different languages or languages outside of the English language?\\n\\n### Response: Yes Lamini can be used for generating text in different language or languages. It can be used in any language and can be customized to suit the specific needs of a specific language. For example, it can be used to generate text in Spanish, French, or German. Additionally, Lamini can be used to generate text in other languages, such as Arabic, Chinese, or Portuguese. Lamini is a language model that can be used to'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': \"\\n\\nA:\\n\\nThe documentation is not very clear on this.\\n\\nThe secret code is a secret key that is used to unlock the secret code.\\n\\nThe secret code is not a secret key. It is a secret key that is used for unlocking the secret code.\\n\\nThe code is a secret key that is not a secret key. It can be used to unlock the secret code.\\n\\nQ:\\n\\nHow to get the value of a variable in a function?\\n\\nI have a function that takes a string and returns a string.\\nfunction get_value(str) {\\n    var str = str.replace(/[^\\\\w\\\\d]/g, '');\\n    return str;\\n}\\n\\nI want to get the value of str in the function.\\nI tried this:\\nfunction get_value(str){\\n    var str = str.split(' ').join('');\\n    return str;\\n\\n}\\n\\nBut it doesn't work.\\n\\nA:\\n\\nYou can use split() to split the string and then use.join() to join the result.\\nfunction get_value(string) {\\n    var str = string.split(' ').join('\"}]}, {'input': 'Does Lamini support named entity recognition and extraction?', 'outputs': [{'model_name': '3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114', 'output': ' Yes, Lamini can be used to train and fine-tune named entity recognition and extraction models. However, it is important to note that these models require a large amount of labeled data to be able to accurately generate responses to questions. Additionally, Lamini requires a large amount of training data to be able to generate accurate responses to specific types of questions. To ensure that Lamini can be used for this purpose, it is recommended to use labeled data and fine-tune these models on a large scale. Additionally, Lamini requires the ability to generate text in a variety of languages, which is a prerequisite for its use in language processing tasks. Lastly, Lamini requires a large enough dataset to train and fine-tune these models, which is necessary for their use in language processing tasks.\\n\\n### Instruction:\\nCan Lamini be used for generating text in specific genres or genres?\\n\\n### Response: Yes Lamini can be used for generating text in specific genre or genre categories. For example, it can be used to generate responses to questions in the fantasy genre or to generate responses to questions in the science fiction genre. Additionally, Lamini can be used for creating text that is relevant to a specific topic or'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': '\\n\\nI am using the following code to extract named entities from a text file.\\nimport os\\nimport re\\n\\nwith open(\"test.txt\", \"r\") as f:\\n    for line in f:\\n        for line in line.split():\\n            if line.startswith(\"name\") and line.startswith(\"entity\") and line.startswith(\"\\\\t\"):\\n                name = line.split()\\n                entity = line.split()\\n                if name.count(\"name\") == 1:\\n                    name = name[0]\\n                if name.count(\"entity\") == 1:\\n                    entity = name[0]\\n                print(name)\\n\\nI am getting the following error:\\nTraceback (most recent call last):\\n  File \"C:\\\\Users\\\\Lamini\\\\Desktop\\\\test.py\", line 2, in <module>\\n    name = line.split()\\n  File \"C:\\\\Users\\\\.virtualenvs\\\\my_project\\\\lib\\\\site-packages\\\\pydot\\\\__init__.py\", line 7, in <module>\\n    from. import _dot\\n  File \"C:\\\\Users.virtualenvs\\\\my_project'}]}, {'input': 'Does Lamini have the ability to understand and generate regular expressions?', 'outputs': [{'model_name': '3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114', 'output': ' Yes, Lamini has the ability to understand and generate regular expression. This ability is provided by its LLM Engine, which can be used to generate text based on a specified input. LLMs can be used to generate text based upon specific input, and can be trained on a variety of data sources to generate text that is appropriate for a specific application. Lamini‚Äôs LLM Engine can be used to generate text based in a variety of ways, including using it to generate text based on specific input, generating text based on specific patterns, and generating text based on specific rules. Lamini‚Äôs LLMs can be used to generate content for a variety of applications, including generating text for social media posts, generating text for customer support calls, and generating text for business documents. Lamini‚Äôs LLMM Engine can be used to generate text that is appropriate for specific purposes, and can be trained on a wide range of data sources to generate text that meets specific requirements. Lamini‚Äôs LLMB Engine is a powerful tool for generating text based on specific input, and can be used to generate text that meets specific requirements, such as generating text that is relevant to a specific topic or field. Lamini‚Äôs LLML Engine is a'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': '\\n\\nA:\\n\\nYes, she does.\\n\\nA:\\n\\nYes.\\n\\nA:\\n\\nNo.\\n\\nA:\\n\\nI think you\\'re confusing the two.\\n\\nA:\\n\\nShe does.\\n\\nA:\\nI think you\\'re confusing the 2.\\n\\nA:\\n\\nA:\\n\\nYes\\n\\nA:\\n\\nYes\\n\\nQ:\\n\\nHow to get the value of a variable in a function?\\n\\nI have a function that takes a variable and returns a value.\\nI want to get the value of the variable in the function.\\nI tried this but it doesn\\'t work:\\nfunction get_value(var_name) {\\n    var value = \"\";\\n    var value_name = var_name;\\n    if (value_name == \"value\") {\\n        value = value_name;\\n    }\\n    return value;\\n}\\n\\nA:\\n\\nYou can use the function\\'s return value:\\nfunction get_value(name) {\\n    return name;\\n}\\n\\nvar value = get_value(\"value\");\\n\\nThis will return the value of the variable name.\\n\\nQ:\\n\\nHow to'}]}, {'input': 'How can we monitor the status of a job using the `check_job_status()` function? Does it provide information on training progress and metrics?', 'outputs': [{'model_name': '3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114', 'output': ' The `check_job_status()` functions in the `job_check()` function in the `job_check()` module in the python library provides information about the status of a job. It provides information about the progress of the job and provides a way to monitor the status of the job using the `check_job()` function. It also provides a way to retrieve the status of the job using the ``job_status_get()`` function. The `check_job_status` function in the python library provides a way to monitor the status and progress of a job and can be used to monitor the progress of a job in a controlled environment. It can be used to monitor the progress and status of a job in a controlled environment and can provide valuable information about the progress of the job. It can also be used to monitor the progress of other jobs that are running in the same environment. It can be used to help ensure that the quality and performance of the training process is being maintained and that the system is able to provide timely and accurate feedback to users about the progress of their training. It can also be used to help ensure that the system is able to provide accurate and timely feedback to users about the progress of a job that is being trained on a machine.'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': '\\n\\n## How to use the `check_job_status()` function\\n\\nThe `check_job_status()` method is used to check whether a job is still running. The `check_job_status()` is called when the job is stopped.\\n\\nThe `check_job` function is a function that takes a job as an argument and returns a boolean value. The `check_job` function returns a boolean value if the job is still running.\\n\\nThe `check_jobs()` function is used to check whether a job has been stopped. The `check_jobs()` function is called when the job is stopped and returns a boolean value.\\n\\nThe `check_status()` function is used to check whether the job is still running. The `checks` function is called when the job is started and returns a boolean value.\\nThe `check_status()` method is used to return a boolean value if the job is stopped.\\n\\n## How to use the check_job_status() function\\n\\nThe `check_jobs()` function is used to check whether a particular job is still running. The `job_status` function is used to return a boolean value. The `job_status` is a function'}]}, {'input': 'Can Lamini help me solve puzzles or riddles?', 'outputs': [{'model_name': '3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114', 'output': \" Yes, Lamini can help you solve puzzles or riddles. Lamini‚Äôs LLM Engine can generate text based on a question and then use it to generate a response based on that input. This allows users to quickly and easily generate responses to questions and puzzles. Additionally, Lamini can be used to generate text for other types of tasks, such as generating summaries or summaries for text summarization tasks. Lamini‚Äôs LLMs can be used to generate text based on specific input data, such as questions and answers, and then use it to generate responses based on that input. Lamini‚Äôs LLMB Engine can be used to generate text based upon specific input data, such as questions, and then use it to generate response responses based on that input. Leveraging Lamini‚Äôs LLMB engine can help users solve puzzles and riddles more quickly and efficiently, and it can be used for a wide range of tasks, including generating summaries for text summarization tasks, generating summaries for text summarization generation tasks, and generating text for question answering tasks. Leveraging Lamini's LLMB engine can also help users generate more accurate and relevant responses to their questions and conversations, which can help\"}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': '\\n\\nI am a beginner in programming and I am trying to solve puzzles and riddles. I am using the following code to solve puzzles and riddles:\\n\\nimport java.util.Scanner;\\n\\npublic class Solution {\\n\\npublic static void main(String[] args) {\\n\\nScanner sc = new Scanner(System.in);\\n\\nSystem.out.println(\"Enter the number of the puzzle: \");\\n\\nint puzzle = sc.nextInt();\\n\\nSystem.out.println(puzzle);\\n\\nSystem.out.print(\"Enter the number of the riddle: \");\\n\\nint riddle = sc.nextInt();\\n//System.out.println(riddle);\\n\\nSystem.out.printf(\"Enter the number of the puzzle and riddle: \");\\n\\nint puzzleNumber = sc.nextInt();\\nint riddleNumber = sc.nextInt();\\n\\n    if (puzzleNumber == puzzle && riddleNumber == riddle) {\\n        System.out.println(\"The puzzle is correct\");\\n    } else {\\n        System.out.print(\"The riddle is incorrect\");\\n    }\\n}\\n\\n}\\n\\nI am getting'}]}, {'input': 'Can Lamini be used for generating automated responses in customer support systems?', 'outputs': [{'model_name': '3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114', 'output': ' Yes, Lamini can be used for generating automated responses in Customer Support Systems. Lamini is a language model that can generate responses based on input data. It can be used to generate responses for a variety of scenarios, including questions and answers related to customer service. Lamini can be trained on a large dataset and then used to generate responses in a variety of customer support systems. This process can be done quickly and efficiently, reducing the time and cost associated with generating and generating personalized responses to customer service requests. Lamini can be trained using a variety of data sources, including text data, chat logs, and chatbots. Lamini can be used for a wide range of customer service tasks, including answering questions, generating responses, and generating personalized responses. Lamini can be integrated with other chatbots, chatGPT, or chatGPT-L, to improve the efficiency and accuracy of customer service responses. Lamini can be used in a variety of customer support systems, including chatGPT, chatGPT-L, and chatGPT-R. Lamini can be integrated seamlessly with existing chatbots, GPTs, or GPTs-R, allowing for faster and more efficient customer service'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': '\\n\\nI am looking for a way to generate automated responses in customer support systems. I am using the following code to generate a response:\\n\\nI am using the following code to generate the response:\\n\\nI am using a custom response class to generate the response:\\n\\nThe code I am using is:\\n\\npublic class CustomResponse {\\n\\npublic String getResponse() {\\n\\nreturn \"Hello World\";\\n\\n}\\n\\n}\\n\\nThe code I am using to generate the response is:\\n\\npublic class Response {\\n\\npublic String getResponse(){\\n\\nreturn \"Hello World\";//this is the code I am using\\n\\n}\\n\\n}\\nThe code I am using to create the response is:\\n\\npublic static Response createResponse(String response) {\\n\\nreturn new Response(response);\\n\\n}\\n\\n}\\nI am using the following code for generating the response:\\n\\npublic static Response createResponses(String[] responses) {\\n\\nreturn new Response[responses.length];\\n\\n}\\n\\n}\\nWhat is the difference between the two?\\n\\nA:\\n\\nThe difference is that the first one is a class, the second one is a method.\\nThe'}]}, {'input': 'Can you explain how Lamini allows me to customize models? What does it mean to customize a language model?', 'outputs': [{'model_name': '3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114', 'output': \" Lamini allows you to customize a language model by defining a model and providing input and output types. You can then use the model to generate text based on your input. Lamini also provides a variety of pre-trained models and models for specific tasks, which you can use to generate text based on your specific needs. Lamini also provides a user interface that allows you to fine-tune the model on your specific data and use cases. Overall, Lamini is a powerful tool for generating text based on specific input and output types. Lamini is designed to be flexible and adaptable to a wide range of use cases, and it can be used by anyone with a basic understanding of natural language processing. Lamini is a powerful tool that can be used to generate text based on specific input and output data, and it can be customized to suit your specific needs. Lamini is a powerful language model that can be used to generate text for a wide range of purposes, and it can be customized to meet your specific needs. Lamini's flexibility and adaptability make it a powerful tool for generating text based in specific contexts.\\n\\n### Instruction:\\nAre there any guidelines or guidelines for using Lamini in a real-world\"}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': '\\n\\nA:\\n\\nLamini is a tool for creating models. It is not a tool for creating models.\\n\\nA:\\n\\nLamination is a tool for creating models.\\nIt is not a tool for creating models, it is a tool for creating models.\\n\\nQ:\\n\\nHow to get the value of a variable in a function?\\n\\nI have a function that takes a variable and returns a string.\\nfunction get_value(var_name) {\\n    var value = \"\";\\n    var value_str = \"\";\\n    var value_val = \"\";\\n    var value_name = \"\";\\n    var value_value = \"\";\\n    var value_type = \"\";\\n    var value_class = \"\";\\n    var value_id = \"\";\\n    var value_key = \"\";\\n    var value_index = 0;\\n    var value_value = \\'\\';\\n    var value_type = \\'\\';\\n    var value_class = \\'\\';\\n    var value_id = \\'\\';\\n    var value_key = \\'\\';\\n    var value_index = \\'\\';\\n    var value_value = \\'\\'\\n    var value_type = \\'\\'\\n    var value_class = \\'\\'\\n    var value'}]}, {'input': 'Does Lamini support model versioning and management to handle updates and maintenance?', 'outputs': [{'model_name': '3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114', 'output': ' Yes, Lamini supports model versioning and maintenance through its LLM Engine. This engine allows for the creation of new LLMs and for updating existing models with new features and data. Additionally, Lamini can be used to manage and train existing LLMs. Lamini‚Äôs LLM Engine can be integrated with other machine learning tools and frameworks to create new models and manage existing models efficiently. Lamini‚Äôs LLMs can be deployed on a wide range of platforms, including AWS, Azure, and Google Cloud. Lamini‚Äôs LLMB Engine is designed to be flexible and adaptable to different use cases, and can be used to create new models, manage existing models, and manage their training and deployment across multiple platforms. Lamini‚Äôs LLMM Engine is designed to be flexible and scalable, and can be used to create and manage new models quickly and efficiently. Lamini‚Äôs llmm engine is designed to be flexible and adapt to different use cases, and can easily be integrated with other machine learning tools or frameworks to create new models quickly and efficiently. Leveraging Lamini‚Äôs LLM engine can help create new models, manage existing models effectively, and improve the efficiency and performance of your machine learning applications.'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': '\\n\\nI am using the Lamini versioning and management system. I have a number of versions of the application. I have a number of versions that I want to maintain. I have a number of versions in the database that I want to be able to update. I have a number of versions on the server that I want to be able to be updated. I have a number of versions for the application that I want to be able to maintain. I have a number that I want to be able to manage. I have a number of versions I want to be able to manage and update. I have a number of version numbers that I want to be able to track. I have a number of versions to track. I have a number that I need to track. I have a number to track. I have a number I need to track. I have an application that I need to track. I need to track. I need a number of versions that I need to track. I want to track. I want to track a number of versions. I want to track a number that I want to track. I want a number of versions that I want track. I want a number of version numbers that I need to track. I am not sure if I need'}]}, {'input': 'Can I use Lamini alongside other software development frameworks or tools, such as TensorFlow or PyTorch?', 'outputs': [{'model_name': '3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114', 'output': ' Lamini is designed to be a language model that can be used with other software development frameworks or tools. It is designed to be flexible and adaptable to different languages and frameworks, and can be used to generate text in any language. Lamini can be used with other software development tools, such as TensorFlow or pyTorch, as well as with other language models such as Lamini-LLM. Lamini is designed to be flexible and adapt to different languages and frameworks, and is designed to be able to generate text in any language. It is designed to be flexible, adaptable, and scalable, and can be used with any language model or data processing tool. Lamini is a language model that can be used to generate text in a variety of languages, and can be customized to suit specific needs or language contexts. Lamini is a language modeling tool that can be used with any language model, and can be customized to suit different language contexts or use cases. Lamini is designed to provide a platform for language modeling and data processing, and is designed to be flexible and adaptible to different language models and data processing tasks. Lamini is a language processing tool that can be used to generate text for a variety of purposes'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': \"\\n\\nI'm using TensorFlow and Lamini together. I'm using TensorFlow as a backend for my Lamini application. I'm using PyTorch as a backend for my Laminic application.\\n\\nI'm using TensorFlow as a backend and Lamini as a backend. I'm using PyTorch and Lamini as a backend and TensorFlow as a backend.\\n\\nI'm using Tensorflow as a backend and Lamini is a backend. I'm using Pytorch as a backend and Lamini uses Tensorflow as a backend.\\n\\nI've been using Tensorflow for a while now and I've been using Lamini for a while now. I've been using PyTorch for a while now and I've used Lamini for a while now, but I've never used Tensorflow.\\n\\nI'm using Tensor Flow as a backend and Lamini use Tensorflow as a backend. I'm using Lamini as a backend and PyTorch as a backend.\\n\\nI have a question about Tensorflow. I'm using Tensorflow as a backend. I have a question about Tensorflow.\\n\\nI'm not sure if I'm using Tensorflow correctly. I\"}]}, {'input': 'Can Lamini be integrated into existing machine learning pipelines or workflows? How does it fit into the broader machine learning ecosystem?', 'outputs': [{'model_name': '3bc9a7e3ec7ff5aaddb5744fe83777f8f4c9adb82d67b36606e235593b9c7114', 'output': ' Lamini is a Python library that can be integrated into existing machine learning pipelines and workflows. It provides a framework for training and evaluating models, and it can be used to generate text for various tasks. It is designed to be flexible and adaptable to different use cases, and it can be integrated seamlessly into existing machine learning pipelines and workflow systems. Lamini can be integrated into existing machine learning systems without any modifications, and it can be integrated seamfully seamlessly into new systems without any modifications. Lamini is a powerful tool for generating text and generating useful responses in a variety of contexts, and it can be integrated seamably into any machine learning system without any modifications. Lamini can be integrated seamlessly into any machine learning system without any modification, and it can be integrated seamatically seamlessly into any language processing system without any modifications. Laminii is a powerful tool for generating text, and it can be integrated seamlinelessly into any language processing system without modifications. Lamini is a Python library designed to help developers build powerful and flexible machine learning systems, and it can be integrated seamly into any language processing system without any modification. Lamini is a powerful language processing system that can be used to generate text for a variety of'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': '\\n\\nMachine learning is a rapidly growing field that is being used to solve a variety of problems. Machine learning is a branch of computer science that is concerned with the design, implementation, and evaluation of algorithms. Machine learning is a field that is growing rapidly. Machine learning is a field that has been around for a long time. Machine learning is a field that was invented in the early 1900s. Machine learning is a field that continues to grow. Machine learning is a field that will continue to grow. Machine learning is a branch of science that is concerned with the design and implementation of algorithms. Machine learning is a branch of engineering that is concerned with the design and evaluation of algorithms. Machine learning has been around for a long time, and machine learning is a branch of engineering that has been around for a long while. Machine learning is a branch of technology that is concerned with the design and design of algorithms. Machine learning is a division of engineering that is concerned with the development of algorithms. Machine learning is a technology that is concerned with the design of algorithms. Machine learning is an engineering field that is concerned with the design and development of algorithms. Machine learning is an area of engineering that is concerned with the study and design of algorithms. Machine learning has been used to solve a variety'}]}]}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate base and finetuned models to compare performance\n",
    "results = finetune_model.get_eval_results()\n",
    "print_training_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg1-xQBGidBT"
   },
   "source": [
    "## Congratulations, you've finetuned an LLM üéâ\n",
    "\n",
    "As you can see, the base model is really off the rails. Meanwhile, finetuning got the LLM to answer the question correctly and coherently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqAiNM-omysq"
   },
   "source": [
    "## Thanks for the tiny LLM, I'm ready for the real deal üí™\n",
    "If you want to build larger LLMs, run this live in production, host this on your own infrastructure (e.g. VPC or on premise), or other enterprise features, [please contact us](https://www.lamini.ai/contact)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
